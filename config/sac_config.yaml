results_dir: results

Env:
  opponent: weak  # weak, strong, self_play
  self_play_period: 1000  # epochs

SAC:
  lr_pi: 0.0005
  lr_q: 0.001
  init_alpha: 0.01
  gamma: 0.98
  batch_size: 32
  buffer_limit: 50000
  start_buffer_size: 1000
  train_iterations: 20
  tau: 0.01  # for target network soft update
  target_entropy: -1.0  # for automated alpha update
  lr_alpha: 0.001  # for automated alpha update
  action_magnitude: 1

actor:
  architecture: [128, 128]
  activation_function: ReLU
  latent_dim: 128
